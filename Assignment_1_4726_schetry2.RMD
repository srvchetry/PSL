---
title: "Assignment_1_4726_schetry2"
author: "Saurav Prem Kaushik Chetry"
date: "09/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(4726)
library("class")
library("ggplot2")
```


```{r}
#variable and vector definitions.

num_sim = 20
myk = rep(0,num_sim)
train.err_lr = rep(0,num_sim)
test.err_lr = rep(0,num_sim)
train.err_qr = rep(0,num_sim)
test.err_qr = rep(0,num_sim)
train.err.knn = rep(0,num_sim)
test.err.knn = rep(0,num_sim)
train.err.Bayes = rep(0,num_sim)
test.err.Bayes = rep(0,num_sim)



#20 simulations with each generating new data and centers. Each iteration calculates the class

for(i in 1:num_sim){
  
  #Generate the 20 centers, 10 for each group.
  
  csize = 10;       # number of centers
  p = 2;      
  s = 1;      # sd for generating the centers within each class                    
  m1 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(1,csize), rep(0,csize));
  m0 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(0,csize), rep(1,csize));
  
  
  #Generate training data.
  
  n=100;  
  # Randomly allocate the n samples for class 1  to the 10 clusters
  id1 = sample(1:csize, n, replace = TRUE);
  # Randomly allocate the n samples for class 0 to the 10 clusters
  id0 = sample(1:csize, n, replace = TRUE);  
  
  # sd for generating x.
  s= sqrt(1/5);                                
  
  traindata = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
  #dim(traindata)
  Ytrain = c(rep(1,n), rep(0,n))
  
  #Generate test data.
  
  N = 5000;  
  id1 = sample(1:csize, N, replace=TRUE);
  id0 = sample(1:csize, N, replace=TRUE); 
  testdata = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id1,], m0[id0,])
  #dim(testdata)
  Ytest = c(rep(1,N), rep(0,N))
  
  #Linear Regression
  
  data_train = data.frame(traindata,Ytrain);
  #dim(data_train)
  data_test = data.frame(testdata, Ytest);
  #dim(data_test)
  
  lr = lm(Ytrain~ X1 + X2, data_train)
  
  Ytrain_hat = as.numeric(lr$fitted > 0.5)
  train.err_lr[i] = mean((Ytrain-Ytrain_hat)^2)

  
  Ytest_hat = as.numeric(predict(lr, data_test)>0.5)
  test.err_lr[i] = mean((Ytest-Ytest_hat)^2)

  
  #Quadratic Regression
  
  qr = lm(Ytrain~X1 + X2 + I(X1 * X2) + I(X1^2) + I(X2^2),data = data_train)
  
  Ytrain2_hat = as.numeric(qr$fitted > 0.5)
  train.err_qr[i] = mean((Ytrain-Ytrain2_hat)^2)
  

  
  Ytest2_hat = as.numeric(predict(qr, data_test)>0.5)
  test.err_qr[i] = mean((Ytest-Ytest2_hat)^2)
  

  

  cvKNN = function(dataSet, foldNum) {
    foldSize = floor(nrow(dataSet)/foldNum)
    KVector = seq(1, (nrow(dataSet) - foldSize), 2)
    cvKNNAveErrorRates = rep(0,length(KVector))
    myIndex = sample(1:nrow(dataSet))
    
    for(k in 1:length(KVector)){
      error = 0
      for (runId in 1:foldNum) {
        testSetIndex = ((runId - 1) * foldSize + 1):(ifelse(runId == 
                                                              foldNum, nrow(dataSet), runId * foldSize))
        testSetIndex = myIndex[testSetIndex]
        trainX = dataSet[-testSetIndex, c("X1", "X2")]
        trainY = as.factor(dataSet[-testSetIndex, ]$Y)
        testX = dataSet[testSetIndex, c("X1", "X2")]
        testY = as.factor(dataSet[testSetIndex, ]$Y)
        predictY = knn(trainX, testX, trainY, KVector[k])
        error = error + sum(predictY != testY)
        
      }
      
      cvKNNAveErrorRates[k] = error/nrow(dataSet)
      
    }
    #print(cvKNNAveErrorRates)
    
    result = list()
    result$bestK = max(KVector[cvKNNAveErrorRates == min(cvKNNAveErrorRates)])
    result$cvError = cvKNNAveErrorRates[result$bestK]
    result
  }
  KNNResult = cvKNN(data_train,10)
  myk[i] = KNNResult$bestK
  
  #Bayes Error
  
  mixnorm=function(x){
    ## return the density ratio for a point x, where each 
    ## density is a mixture of normal with 10 components
    sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
  }
  

  Ytrain_pred_Bayes = apply(traindata, 1, mixnorm)
  Ytrain_pred_Bayes = as.numeric(Ytrain_pred_Bayes > 1);
  #table(Ytest, Ytest_pred_Bayes); 
  train.err.Bayes[i] = sum(Ytrain !=  Ytrain_pred_Bayes) / (2*n)
  
  
  
  Ytest_pred_Bayes = apply(testdata, 1, mixnorm)
  Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1);
  #table(Ytest, Ytest_pred_Bayes); 
  test.err.Bayes[i] = sum(Ytest !=  Ytest_pred_Bayes) / (2*N)
  
 
  
}

m = length(myk)

train.err.knn = rep(0,m)
test.err.knn = rep(0,m)

for( j in 1:m){
  Ytrain.pred = knn(traindata, traindata, Ytrain, k = myk[j])
  train.err.knn[j] = sum(Ytrain !=  Ytrain.pred) / (2*n)
  Ytest.pred = knn(traindata, testdata, Ytrain,k = myk[j])
  test.err.knn[j] = sum(Ytest !=  Ytest.pred) / (2*N)
}

```   

```{r}
 
boxplot(train.err_lr,test.err_lr,train.err_qr,test.err_qr,train.err.knn,test.err.knn,train.err.Bayes,test.err.Bayes,las = 2,names = c("Linear_tr","Linear_te","Quadr_tr","Quad_te","KNN_tr","KNN_te","Bayes_tr","Bayes_te"),ylab = "ErrorRate", main = "Train-Test ErrorRates Vs Methods", sub = "Methods")

```

```{r}
#barplot of the best K selected in each of the 20 simulations
barplot(myk, xlab = "best K selected by 10 fold CV for KNN in each simulation", ylab = "K")

#mean of the best Ks selected
mean(myk)

#standard error of the selected best Ks
sd(myk)
```


```{r}
#Performance Plot

plot(c(0.5,m), range(test.err_lr, train.err_lr,test.err_qr,train.err_qr, test.err.knn, train.err.knn),
     type="n", xlab="Degree of Freedom", ylab="Error", xaxt="n")
myk = sort(myk,decreasing = TRUE)
df = round((2*n)/myk)
axis(1, at = 1:m, labels = df)
axis(3, at = 1:m, labels = myk)

points(1:m, test.err.knn, col="red", pch=1);
lines(1:m, test.err.knn, col="red", lty=1);
points(1:m, train.err.knn, col="blue", pch=1);
lines(1:m, train.err.knn, col="blue", lty=2);

points(3, mean(train.err_lr), pch=2, cex=2, col="blue")
points(3, mean(test.err_lr), pch=2, cex=2, col="red")

points(6, mean(train.err_qr), pch=1, cex=2, col="blue")
points(6, mean(test.err_qr), pch=1, cex=2, col="red")

abline(test.err.Bayes, 0, col="purple")

mtext("Best K from each simulation", side=3, line=3, cex.lab=1)

legend("topright", legend = c("Test_KNN", "Train_KNN", "Bayes", "Test_LR", "Train_LR","Test_QR","Train_QR"), lty = c(1,2,1,NA,NA,NA,NA), pch = c(NA,NA,NA,2,2,1,1), col = c("red", "blue", "purple", "red", "blue","red","blue"), cex = 0.7)

```

